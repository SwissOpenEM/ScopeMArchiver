{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ScopeMArchiver","text":"<p>An archiver service that allows uploading dataset and registering it with SciCat. It is built on Prefect.io to orchestrate the asynchronous jobs (called flows) to archive and retrieve datasets to/from the ETH LTS.</p> <p>The full setup is containerized and requires a SciCat instance to run correctly.</p> <p>Refer to the Github pages for more details.</p>"},{"location":"#quick-start","title":"Quick start","text":"<p>Build all the services:</p> <pre><code>docker compose --env-file .env.prod --env-file .env.development build\n</code></pre> <p>Starting up all services for development:</p> <pre><code>docker compose --env-file .env.prod --env-file .env.development up -d\n</code></pre>"},{"location":"#deploy-local-flows","title":"Deploy Local Flows","text":"<p>For development and debugging, a local process can serve flows, for example by running <code>python -m archiver.flows</code>. However, some more configuration is required to fully integration with the other services; therefore a VS Code launch command <code>Prefect Flows</code> can be used in launch.json). This allows to debug flows locally and the registered flows have a prefix, <code>DEV_</code>.</p>"},{"location":"#github-pages","title":"Github Pages","text":"<p>The latest documentation can be found in the GitHub Pages</p>"},{"location":"#bulding-github-pages","title":"Bulding Github Pages","text":"<p>Github pages are built on mkdocs.</p> <p>Build docker image first:</p> <pre><code>docker build . -f docs.Dockerfile -t ghcr.io/swissopenem/scopemarchiver-docs:latest\n</code></pre> <p>Start development server</p> <pre><code>docker run --rm -it -p 8000:8000 -v ${PWD}:/docs ghcr.io/swissopenem/scopemarchiver-docs:latest\n</code></pre> <p>Build documentation</p> <pre><code>docker run --rm -it -p 8000:8000 -v ${PWD}:/docs ghcr.io/swissopenem/scopemarchiver-docs:latest build\n</code></pre> <p>Deploy documentation to GitHub Pages</p> <pre><code>docker run --rm -it -v ~/.ssh:/root/.ssh -v ${PWD}:/docs ghcr.io/swissopenem/scopemarchiver-docs:latest gh-deploy\n</code></pre>"},{"location":"#authentication-and-securing-of-dashboards","title":"Authentication and securing of dashboards","text":"<p>Certain service pages do not support standard OAuth2/OIDC authentication mechanism, such as:</p> <ul> <li>Traefik dashboard</li> <li>MinIO admin login</li> <li>Grafana monitoring dashboard</li> </ul> <p>To protect these pages, we use a proxy technology called Forward Auth. To achieve this, we need to configure the following:</p> <ul> <li>we need to tell Traefik to act as a reverse proxy to logically protect certain webpages</li> <li>we do this by registering a so called middleware in traefik</li> <li>this middleware will be of type forwardauth and redirects to a service called authentik-proxy, e.g. <code>traefik.http.middlewares.authentik.forwardauth.address: http://authentik-proxy:9000/outpost.goauthentik.io/auth/traefik</code></li> <li><code>authentik-proxy</code> service integrates with Traefik's reverse proxy and acts as a authentication gateway to Authentik's outpost</li> <li>an outpost in Authentik is a deployment that acts as a bridge between Authentik and external services, handling authentication and authorization. </li> <li>in Authentik (our identity broker) we need to register</li> <li>Application: basically a name and a URL where the application can be found. It is onnected to one provider</li> <li>Provider: the mechanism we use how to provide authentication and source of the users. For our use-case, we take a Proxy Provider of type Forward Auth at domain level, as all the service pages should be protected the same way. A provider can be connected to many applications.</li> <li>Outpost: an entity that talks to the Authentik-proxy service and provides a AUTHENTIK_TOKEN for that service. It is also connected to the provider. </li> <li>for every service page that we would like to protect, we have to tell Traefik's router to use the <code>authentik</code> middleware defined earlier, e.g. <code>traefik.http.routers.dashboard.middlewares=authentik</code></li> </ul> <p>The following sequence diagram illustrates the authentication mechanism.</p> <pre>\n    \n    \n    \n    \n        \n    \n\n    \n        \n    \n\n    \n        \n    \n\n    \n        \n    \n\n    \n    <code>\nsequenceDiagram\n  autonumber\n  participant U as User Browser\n  participant T as Traefik&lt;br/&gt;(reverse-proxy)\n  participant S as Service Page\n  participant O as authentik-proxy&lt;br/&gt;(outpost)\n  participant AO as Authentik outpost\n  participant AP as Authentik provider\n  participant I as LDAP&lt;br/&gt;(identity provider)\n\n  U -) T: request access to service page\n  T -) T: is user authenticated (cookie \ud83d\udd11)?\n  T -) O: redirect to outpost\n  O -) AO: redirect to authentication (AUTHENTIK_TOKEN \ud83d\udd11)\n  AO -) AP: redirect to login page\n  AP -) U: ask for username\n  U -) AP: enter username\n  AP -) I: check username exists\n  I -) AP: OK\n  AP -) I: authorization: username is in allowed groups\n  I -) AP: OK\n  AP -) U: ask for password (plus OTP, if needed)\n  U -) AP: enter password \ud83d\udd11\n  AP -) I: check password \ud83d\udd11\n  I -) AP: password OK\n  AP -) O: redirect (cookie \ud83d\udd11)\n  O -) T: redirect (cookie \ud83d\udd11)\n  T -) S: access service page\n\n  loop optional: sync users &amp; groups\n     AP -) I: get users, groups\n     I -) AP: users, groups\n  end\n\n</code> \n    \nHold \"Alt\" / \"Option\" to enable pan &amp; zoom\n\n</pre>"},{"location":"architecture/","title":"Architecture","text":""},{"location":"architecture/#component-diagram","title":"Component Diagram","text":""},{"location":"architecture/#components","title":"Components","text":""},{"location":"architecture/#archiver-service","title":"Archiver Service","text":"Name Technology Description Endpoint Reverse Proxy Traefik https://traefik.io/traefik/ Routes traffic to endpoints https://${HOST}/dashboard/ Backend Api FastAPI https://fastapi.tiangolo.com Endpoint for Scicat backend, requests flow scheduling by Prefect https://${HOST}/archiver/api/v1/docs Workflow Orchestraction Prefect https://www.prefect.io Orchestrates workflows for archival and retrieval operations https://${HOST}/archiver/prefect/ui/dashboard"},{"location":"architecture/#workflow-orchestration","title":"Workflow Orchestration","text":"Name Technology Description Endpoint Prefect Server https://docs.prefect.io/3.0/manage/self-host https://${HOST}/archiver/prefect/ui/dashboard Prefect Worker https://docs.prefect.io/3.0/deploy/infrastructure-concepts/workers n/a Prefect Flow https://docs.prefect.io/3.0/develop/write-flows#write-and-run-flows n/a"},{"location":"architecture/#storage-components","title":"Storage Components","text":"Name Technology Description Endpoint Storage Server Minio https://min.io Storage for datasets that are to be archived or are retrievable http://localhost/minio/ LTS Share NFS Network share ETHZ Long term storage where datasets are stored on and retrieved from n/a"},{"location":"architecture/#external-components","title":"External Components","text":"Name Technology Description Endpoint Ingestor Golang https://github.com/SwissOpenEM/Ingestor Client application to select, ingest, and upload datasets n/a SciCat Frontend Node.js https://scicatproject.github.io Data catalog frontend where datasets are registered and archival/retrieval is triggered https://discovery.psi.ch/, https://${HOST}/ SciCat Backend Node.js https://scicatproject.github.io Data catalog backend where datasets are registered and archival/retrieval is triggered https://dacat.psi.ch/explorer/, https://${HOST}/scicat/backend/explorer"},{"location":"archival_retrieval_flow/","title":"Archival and Retrieval Flow","text":"<p>Archival is split into two subflows, <code>Create datablocks</code> and  <code>Move datablocks to LTS</code>An archival task can contain multiple datasets; for simplicity the case with only one is depicted here.</p>"},{"location":"archival_retrieval_flow/#nomenclature","title":"Nomenclature","text":"Identifier Description Flow Sequence of tasks and subflows Subflow Flow triggered by a parent flow Job A Scicat terminology corresponding to a archival flow; may contain multiple datasets as parameters Archive Flow Subflow that runs archiving for one dataset User Error Scicat terminology for errors triggered by the user, e.g. dataset is incomplete; dataset not found System Error Scicat terminiolgy for unrecoverable (transient) error in the system"},{"location":"archival_retrieval_flow/#critical-sections","title":"Critical Sections","text":"<p>A sequence of tasks that need to succeed atomically (all succeed or all are rolled back) are depicted as critical tasks below. They have a specific error handling associated in order to roll back all the task in that sequence.</p>"},{"location":"archival_retrieval_flow/#retries","title":"Retries","text":"<p>All task interacting with external services (i.e. Scicat) have retries implemented. Since they are not done in a task level but on the http request level, they are not depicted below.</p> <p>Reading from the LTS has retries implemented on a task level and is therefore depicted below.</p>"},{"location":"archival_retrieval_flow/#sequence-diagrams","title":"Sequence Diagrams","text":""},{"location":"archival_retrieval_flow/#archival","title":"Archival","text":"<p>The diagramm below shows the different steps in the archiving flow. Each step executed by the <code>Worker</code> is implemented as its own task (expect steps executed in error handling).</p> <pre>\n    \n    \n    \n    \n        \n    \n\n    \n        \n    \n\n    \n        \n    \n\n    \n        \n    \n\n    \n    <code>sequenceDiagram\n  autonumber\n  participant AS as Archival Service\n  participant L as Landing Zone\n  participant W as Worker\n  participant ST as Staging\n  participant S as SciCat\n  participant LTS as LTS\n\n\n  Note over AS, S: Subflow: Create datablocks\n\n  S --) AS: Archive: POST /api/v1/jobs\n  Note right of AS: Job {\"id\" : \"id\", \"type\":\"archive\", \"datasetlist\": [], ... } as defined in Scicat\n\n  AS --) W: Create archival flow for each dataset\n  AS --&gt;&gt; S: Respond to Scicat\n  Note left of S: {\"uuid\": \"&lt;flow uuid&gt;\", \"name\": \"&lt;flow name&gt;\"}\n\n  W --) S: PATCH /api/v4/Jobs/{JobId}\n  Note left of S: {\"jobStatusMessage\": \"inProgress\"}, \n\n  critical Job for datasetlist\n    Note over L,LTS: Subflow: Move dataset to LTS\n  critical Subflow for {DatasetId}\n              W --) S:  PATCH /api/v4/Dataset/{DatasetId}\n              Note left of S: {\"datasetlifecycle\": {\"archiveStatusMessage\": \"started\"}}  \n              W --&gt; S: GET /api/v4/Datasets/{dataset_id}/origdatablocks\n        W --&gt;&gt; L: Create datablocks\n        L --&gt;&gt; ST: Move datablocks to staging\n        W --&gt;&gt; S: Register datablocks POST /api/v4/Datasets/{DatasetId}/datablocks\n        W --&gt;&gt; L: Cleanup dataset files, datablocks\n        Note right of L: Dataset files only get cleaned up when everythings succeeds\n    option Failure user error\n        W --) S: Report error: PATCH /api/v4/Dataset/{DatasetId}\n        Note left of S: {\"datasetlifecycle:{\"archiveStatusMessage\": \"missingFileserror\", \"archivable\": False, \"retrievable\":False}}\n        W --&gt;&gt; ST: Cleanup Datablocks\n        W --&gt;&gt; L: Cleanup Datablocks\n        Note right of L: No cleanup of dataset files\n    option Failure system error\n        W --) S: Report error: PATCH /api/v4/Dataset/{DatasetId}\n        Note left of S: {\"datasetlifecycle:{\"archiveStatusMessage\": \"scheduleArchiveJobFailed\", \"archivable\": \"false\", \"retrievable\":\"false\"}}\n        W --&gt;&gt; ST: Cleanup datablocks\n        W --&gt;&gt; L: Cleanup datablocks\n        Note right of L: No cleanup of dataset files\n    end\n    option Failure user error\n        W --) S: Report error: PATCH /api/v4/Jobs/{JobId}\n        Note left of S: {\"jobStatusMessage\": \"finishedWithDataseterrors\"}\n    option Failure system error\n        W --) S: Report error: PATCH /api/v4/Jobs/{JobId}\n        Note left of S: {\"jobStatusMessage\": \"finishedUnsuccessful\"}\n  end\nNote over W, LTS: Subflow: Move datablocks to LTS\nloop for each datablock \n  critical\n      ST --&gt;&gt; LTS: Move datablock to LTS\n      loop Retry\n        ST --&gt;&gt; LTS: Verify checksum of datablock to LTS with retries\n      end\n  option Moving datablock to LTS Failed\n      W --&gt;&gt; LTS: Cleanup LTS folder\n      W --&gt;&gt; S: Report error\n      Note left of S: {\"jobStatusMessage\": \"scheduleArchiveJobFailed\"}\n  end\n  critical\n      W --&gt;&gt; LTS: Verify datablock in LTS\n      W --&gt;&gt; L: Cleanup dataset files, datablock\n  option Datablock validation failed\n      W --&gt;&gt; S: Report error\n      Note left of S: {\"jobStatusMessage\": \"scheduleArchiveJobFailed\"}\n      W --&gt;&gt; LTS: Cleanup datablocks\n  end\nend\n    W --&gt;&gt; S:  PATCH /api/v4/Datasets/{DatasetId}\n    Note left of S: {\"datasetlifecycle\": {\"archivable\": False, \"retrievable\": True, \"archiveStatusMessage\": \"datasetOnArchiveDisk\"}} \n    W --&gt;&gt; S: PATCH /api/v4/Jobs/{JobId}\n    Note left of S: {\"jobStatusMessage\": \"finishedSuccessful\"} \n</code> \n    \nHold \"Alt\" / \"Option\" to enable pan &amp; zoom\n\n</pre>"},{"location":"archival_retrieval_flow/#retrieval","title":"Retrieval","text":"<p>The diagramm below shows the different steps in the retrieval flow. Each step executed by the Worker is implemented as its own task (expect steps executed in error handling).</p> <pre>\n    \n    \n    \n    \n        \n    \n\n    \n        \n    \n\n    \n        \n    \n\n    \n        \n    \n\n    \n    <code>sequenceDiagram\n  autonumber\n  participant AS as Archival Service\n  participant J as Worker\n  participant ST as Staging\n  participant S as SciCat\n  participant LTS as LTS\n\n  S --) AS: Retrieve: POST /api/v1/jobs\n  Note right of AS: Job {\"id\" : \"id\", \"type\":\"retrieve\", \"datasetlist\": [], ... } as defined in Scicat\n  AS --) J: Create Retrieval Flow\n  AS --&gt;&gt; S: Respond to Scicat\n  Note left of S: {\"uuid\": \"&lt;flow uuid&gt;\", \"name\": \"&lt;flow name&gt;\"}\n\n    J --&gt;&gt; S: PATCH /api/v4/Jobs/{JobId}\n    Note left of S: {\"jobStatusMessage\": \"inProgress\"}  \n    J --&gt;&gt; S: PATCH /api/v4/Datasets/{DatasetId}\n    Note left of S: {\"datasetlifecycle\": {\"retrieveStatusMessage\": \"started\"}}  \n  critical\n      LTS --&gt;&gt; ST: Download Datablocks from LTS\n  option Retrieval Failure\n      J --) S: Report Error: PATCH /api/v4/Dataset/{DatasetId}\n          Note left of S: {\"retrieveStatusMessage\": datasetRetrievalFailed,&lt;br&gt; \"retrieveReturnMessage\": \"retrievalFailed\"}\n      J --) S: Report Error: PATCH /api/v4/Jobs/{JobId}\n         Note left of S: {\"jobStatusMessage\": \"finishedWithDatasetErrors\",&lt;br&gt; \"jobResultObject\": [JobResultEntry]}\n      J --&gt;&gt; ST: Cleanup Files\n  end\n  critical\n    J --&gt;&gt; ST: Validate Datablocks\n  option Validation Failure\n      J --&gt;&gt; S: Report Error: PATCH /api/v4/Dataset/{DatasetId}\n        Note left of S: {\"retrieveStatusMessage\": datasetRetrievalFailed,&lt;br&gt; \"retrieveReturnMessage\": \"retrievalFailed\"}\n      J --&gt;&gt; ST: Cleanup Files\n  end\n      J --&gt;&gt; S: PATCH /api/v4/Jobs/{JobId}\n      Note left of S: {\"jobStatusMessage\": \"finishedSuccessful\"} \n      J --&gt;&gt; S: PATCH /api/v4/Datasets/{DatasetId}\n      Note left of S: {\"datasetlifecycle\": {\"retrieveStatusMessage\": \"datasetRetrieved\",&lt;br&gt;  \"retrievable\": True }} \n\n</code> \n    \nHold \"Alt\" / \"Option\" to enable pan &amp; zoom\n\n</pre> <p>Note: <code>updatedBy</code> and <code>updatedAt</code> are omitted for brevity but need to be set for every update of the job status and datsetlifecycle as well.</p>"},{"location":"archival_retrieval_flow/#jobresult","title":"JobResult","text":"<p>The result of a retrieval job contains a list of <code>JobResultEntry</code> objects, which contain a url in order for end users to download the datablocks.</p>"},{"location":"archival_retrieval_flow/#jobresultentry","title":"JobResultEntry","text":"<pre><code> {\n    \"datasetId\": \"&lt;datasetId&gt;\",\n    \"name\": \"str\",\n    \"size\": \"str\",\n    \"archiveId\": \"str\",\n    \"url\": \"str\"\n }\n</code></pre>"},{"location":"deployment/","title":"Deployment","text":""},{"location":"deployment/#service-deployment","title":"Service Deployment","text":"<p>The services on the virtual machine can be deployed using a single docker-compose file:</p> <pre><code>docker compose --env-file .env --env-file .development.env up -d\n</code></pre> <p>There are configuration parameters for all the services for production and develoment:</p>"},{"location":"deployment/#production","title":"Production","text":""},{"location":"deployment/#development","title":"Development","text":"<p>For development, it is useful to override some configuration:</p> <p>Note: The <code>lts-mock-volume</code> is a local volume here and not the LTS share.</p>"},{"location":"deployment/#prefect-deployment","title":"Prefect Deployment","text":"<p>Prefect is set up in a slightly non-standard way (with respect to their described use cases). There are two workers deployed (archival/retrieval) that mount the hosts Docker socket in order to create containers at runtime in which the flows run. The flows are baked into the containers and the code is not pulled from any repository (Prefect would allow to, for example, store the code in an S3 bucket). The ETHZ LTS volume is mounted in a Docker volume such that the runtime containers can mount those during startup.</p> <p></p> Name Technology Description Endpoint Prefect Server Workflow orchestration https://www.prefect.io http://localhost/prefect-ui/dashboard Postgres Database Database for Prefect n/a Prefect Worker https://docs.prefect.io/3.0/deploy/infrastructure-concepts/workers n/a Runtime Container runtime.Dockerfile n/a"},{"location":"deployment/#prefect-server","title":"Prefect Server","text":"<p>In order to run Prefect server, variables, secrets and concurrency limits need to be configured.</p>"},{"location":"deployment/#configuration","title":"Configuration","text":"<p>All of the configuration can be done by running</p> <p><code>bash     docker compose --env-file .env --env-file .development.env run --rm prefect-config</code></p> <p>with the appropriate PREFECT_API_URL set.</p>"},{"location":"deployment/#variables","title":"Variables","text":"<p>Variables are used at runtime and are fetched from the server by the flow. External endpoints and other parameters of the flow belong here:</p> <pre><code>[archiver]\nARCHIVER_SCRATCH_FOLDER = \"/tmp/scratch\"\nARCHIVER_TARGET_SIZE_GB = 500\nARCHIVER_LTS_FILE_TIMEOUT_S = 300\nARCHIVER_LTS_WAIT_BEFORE_VERIFY_S = 600\n\n[lts]\nLTS_STORAGE_ROOT = \"/tmp/LTS\"  # Root path where LTS share is mounted\nLTS_FREE_SPACE_PERCENTAGE = 20 # Minimum free space percentage of the LTS before archiving task starts\n\n[minio]\nMINIO_REGION = \"eu-west-1\"                        # S3 region\nMINIO_RETRIEVAL_BUCKET = \"retrieval\"              # S3 bucket where datasets are retrieved to\nMINIO_LANDINGZONE_BUCKET = \"landingzone\"          # S3 bucket where datasets are uploaded to  \nMINIO_STAGING_BUCKET = \"staging\"                  # S3 internally used bucket where datasets are staged before copying to LTS\nMINIO_ENDPOINT = \"scopem-openemdata.ethz.ch:9090\" # S3 endpoint of storage server\nMINIO_URL_EXPIRATION_DAYS = 7\n\n[scicat]\nSCICAT_ENDPOINT = \"https://scopem-openem.ethz.ch/scicat/backend\"\nSCICAT_API_PREFIX = \"/api/v3\" # Route prefix of Scicat instance\nSCICAT_DATASETS_API_PREFIX = \"/api/v3\" # Route prefix of Scicat dataset endpoints\nSCICAT_JOBS_API_PREFIX = \"/api/v3\" # jobs endpoints for jobs endpoints\n</code></pre>"},{"location":"deployment/#concurrrency-limits","title":"Concurrrency Limits","text":"<p>There are certain sections of the code (tasks) that can only run in a limited manner concurrently (i.e. writing to the LTS), see https://docs.prefect.io/3.0/develop/task-run-limits#limit-concurrent-task-runs-with-tags.</p> <pre><code>LTS_FREE_LIMIT = 1\nLTS_WRITE_LIMIT = 2\nLTS_READ_LIMIT = 1\n</code></pre>"},{"location":"deployment/#internal-secrets","title":"Internal Secrets","text":"<p>Internal secrets can be created at deployment time.</p> <pre><code># Postgres\necho \"postgres_user\" &gt; .secrets/postgresuser.txt\nopenssl rand -base64 12 &gt; .secrets/postgrespass.txt # creates random string\n</code></pre>"},{"location":"deployment/#external-secrets","title":"External Secrets","text":"<p>The Minio deployment might already provide its own secrets and can be added manually in the UI too.</p> <pre><code># Minio\necho \"minioadminuser\" &gt; .secrets/miniouser.txt\nopenssl rand -base64 12 &gt; .secrets/miniopass.txt # creates random string\n\n// TODO: Needed ?\n# Github\necho \"&lt;github_user&gt;\" &gt; .secrets/githubuser.txt\necho \"&lt;github_access_token&gt;\" &gt; .secrets/githubpass.txt\n</code></pre> Name Description github-openem-username Username for Github container registry github-openem-access-token Personal access token to Github container registry"},{"location":"deployment/#prefect-worker","title":"Prefect Worker","text":"<p>Workers can only be deployed on a machine that has access to</p> <ul> <li>the Prefect server (no authentication implemented in Prefect for on-premise deployement currently)</li> <li>the S3 storage</li> <li>the ETHZ LTS share (ip whitelisting within ETHZ network)</li> </ul> <p>They can be started by the following command:</p> <pre><code>docker compose --env-file .env --env-file .development.env up -d prefect-archival-worker\ndocker compose --env-file .env --env-file .development.env up -d prefect-retrieval-worker\n</code></pre> <p>Note: due to a bug in Prefect the workers concurrency limit needs to be set manually in the UI.</p>"},{"location":"deployment/#flows","title":"Flows","text":"<p>The flows can be deployed using a container:</p> <pre><code>docker compose --env-file .env --env-file .development.env run --rm prefect-flows-deployment\n</code></pre> <p>This deploys the flows as defined in the prefect.yaml and requires the secrets set up in the previous step.</p>"},{"location":"upload_flow/","title":"Ingestion flow","text":"<pre>\n    \n    \n    \n    \n        \n    \n\n    \n        \n    \n\n    \n        \n    \n\n    \n        \n    \n\n    \n    <code>sequenceDiagram\n  autonumber\n  participant C as Client\n  participant S as Scicat\n  participant A as Archiver Service\n\n    C --&gt; C: Select Dataset\n    C --) S: Register Dataset: POST /Datasets\n   activate S \n      Note left of S: { \"isOnCentralDisk\" : false, \"archivable\" : false, \"archiveStatusMessage: \"filesNotReadyYet\"}\n      S --) C: PID\n   deactivate S\n    C --&gt;&gt; A: Upload dataset\n      Note left of A: Dataset\n    C --&gt;&gt; S: PUT /Datasets/{datasetId}\n      Note left of S: { \"datasetlifecycle\": {\"archiveStatusMessage\" : \"datasetCreated, \"archivable\" : false, \"archiveStatusMessage: \"filesNotReadyYet\"}}\n    C --&gt;&gt; S: POST /api/v4/OrigDatablocks\n      Note left of S: {fileBlock, \"datasetId\": \"datasetId\"}\n    S --&gt;&gt; A: Trigger Archive Job: POST /api/v1/Job\n      Note left of A: {}\n\n</code> \n    \nHold \"Alt\" / \"Option\" to enable pan &amp; zoom\n\n</pre> <ul> <li>Source folder never used: why?</li> <li>Datafile list vs origdatablocks?</li> </ul>"},{"location":"reference/Summary/","title":"Summary","text":"<ul> <li>config<ul> <li>blocks</li> <li>concurrency_limits</li> <li>variables</li> </ul> </li> <li>flows<ul> <li>archive_datasets_flow</li> <li>flow_utils</li> <li>mock_flows</li> <li>retrieve_datasets_flow</li> <li>task_utils</li> <li>tests<ul> <li>conftest</li> <li>helpers</li> <li>scicat_unittest_mock</li> <li>test_archival_flow</li> <li>test_retrieval_flow</li> </ul> </li> </ul> </li> <li>scicat<ul> <li>scicat_interface</li> <li>scicat_tasks</li> </ul> </li> <li>tests<ul> <li>scicat_service_mock</li> <li>test_e2e</li> <li>test_scicat_tasks</li> </ul> </li> <li>utils<ul> <li>datablocks</li> <li>log</li> <li>model</li> <li>s3_storage_interface</li> <li>script_generation</li> <li>tests<ul> <li>test_datablocks</li> <li>test_s3</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/config/blocks/","title":"Blocks","text":""},{"location":"reference/config/concurrency_limits/","title":"Concurrency limits","text":""},{"location":"reference/config/variables/","title":"Variables","text":""},{"location":"reference/config/variables/#config.variables.PrefectVariablesModel","title":"<code>PrefectVariablesModel</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Pydantic model of the Prefect Variables config</p> <p>Parameters:</p> Name Type Description Default <code>BaseSettings</code> <code>_type_</code> <p>description</p> required Source code in <code>backend/archiver/config/variables.py</code> <pre><code>class PrefectVariablesModel(BaseSettings):\n    \"\"\"Pydantic model of the Prefect Variables config\n\n    Args:\n        BaseSettings (_type_): _description_\n\n    \"\"\"\n\n    model_config = SettingsConfigDict(\n        env_file_encoding=\"utf-8\",\n        case_sensitive=True,\n        extra=\"forbid\",\n        validate_default=True,\n    )\n\n    MINIO_REGION: str = \"\"\n    MINIO_RETRIEVAL_BUCKET: str = \"\"\n    MINIO_STAGING_BUCKET: str = \"\"\n    MINIO_LANDINGZONE_BUCKET: str = \"\"\n    MINIO_ENDPOINT: str = \"\"\n    MINIO_EXTERNAL_ENDPOINT: str = \"\"\n    MINIO_URL_EXPIRATION_DAYS: int = 7\n\n    LTS_STORAGE_ROOT: Path = Path(\"\")\n    LTS_FREE_SPACE_PERCENTAGE: float = 20\n\n    ARCHIVER_SCRATCH_FOLDER: Path = Path(\"\")\n    ARCHIVER_TARGET_SIZE_GB: int = 20\n    ARCHIVER_LTS_FILE_TIMEOUT_S: int = 60\n    ARCHIVER_LTS_WAIT_BEFORE_VERIFY_S: int = 180\n    ARCHIVER_NUM_WORKERS: int = 4\n\n    SCICAT_ENDPOINT: str = \"\"\n    SCICAT_API_PREFIX: str = \"\"\n    SCICAT_DATASETS_API_PREFIX: str = \"\"\n    SCICAT_JOBS_API_PREFIX: str = \"\"\n</code></pre>"},{"location":"reference/config/variables/#config.variables.Variables","title":"<code>Variables</code>","text":"<p>Singleton abstracting access to all Variables expected to be defined in Prefect</p> Source code in <code>backend/archiver/config/variables.py</code> <pre><code>class Variables:\n    \"\"\"Singleton abstracting access to all Variables expected to be defined in Prefect\"\"\"\n\n    _instance = None\n\n    def __new__(cls):\n        if cls._instance is None:\n            cls._instance = super(Variables, cls).__new__(cls)\n        return cls._instance\n\n    def __get(self, name: str) -&gt; str:\n        c = None\n\n        # try for local overrides as env variables\n        c = os.environ.get(name.upper())\n        if c is not None:\n            return c\n\n        try:\n            c = Variable.get(name)\n        finally:\n            if c is None:\n                getLogger().warning(f\"Value {name} not found in config, returning empty string\")\n                return \"\"\n            return c\n\n    @property\n    def SCICAT_ENDPOINT(self) -&gt; str:\n        return self.__get(\"scicat_endpoint\")\n\n    @property\n    def SCICAT_API_PREFIX(self) -&gt; str:\n        return self.__get(\"scicat_api_prefix\") or \"\"\n\n    @property\n    def SCICAT_DATASETS_API_PREFIX(self) -&gt; str:\n        return self.__get(\"scicat_datasets_api_prefix\") or \"\"\n\n    @property\n    def SCICAT_JOBS_API_PREFIX(self) -&gt; str:\n        return self.__get(\"scicat_jobs_api_prefix\") or \"\"\n\n    @property\n    def MINIO_RETRIEVAL_BUCKET(self) -&gt; str:\n        return self.__get(\"minio_retrieval_bucket\")\n\n    @property\n    def MINIO_LANDINGZONE_BUCKET(self) -&gt; str:\n        return self.__get(\"minio_landingzone_bucket\")\n\n    @property\n    def MINIO_STAGING_BUCKET(self) -&gt; str:\n        return self.__get(\"minio_staging_bucket\")\n\n    @property\n    def MINIO_REGION(self) -&gt; str:\n        return self.__get(\"minio_region\")\n\n    @property\n    def MINIO_ENDPOINT(self) -&gt; str:\n        return self.__get(\"minio_endpoint\")\n\n    @property\n    def MINIO_URL_EXPIRATION_DAYS(self) -&gt; int:\n        return int(self.__get(\"minio_url_expiration_days\") or 7)\n\n    @property\n    def MINIO_EXTERNAL_ENDPOINT(self) -&gt; str:\n        return self.__get(\"minio_external_endpoint\")\n\n    @property\n    def ARCHIVER_SCRATCH_FOLDER(self) -&gt; Path:\n        return Path(self.__get(\"archiver_scratch_folder\"))\n\n    @property\n    def ARCHIVER_TARGET_SIZE_GB(self) -&gt; int:\n        return int(self.__get(\"archiver_target_size_gb\") or 200)\n\n    @property\n    def ARCHIVER_LTS_FILE_TIMEOUT_S(self) -&gt; int:\n        return int(self.__get(\"archiver_lts_file_timeout_s\") or 10)\n\n    @property\n    def ARCHIVER_LTS_WAIT_BEFORE_VERIFY_S(self) -&gt; int:\n        return int(self.__get(\"archiver_lts_wait_before_verify_s\") or 30)\n\n    @property\n    def ARCHIVER_NUM_WORKERS(self) -&gt; int:\n        return int(self.__get(\"archiver_num_workers\") or 30)\n\n    @property\n    def LTS_STORAGE_ROOT(self) -&gt; Path:\n        return Path(self.__get(\"lts_storage_root\"))\n\n    @property\n    def LTS_FREE_SPACE_PERCENTAGE(self) -&gt; float:\n        return float(self.__get(\"lts_free_space_percentage\") or 1.0)\n</code></pre>"},{"location":"reference/flows/archive_datasets_flow/","title":"Archive datasets flow","text":""},{"location":"reference/flows/archive_datasets_flow/#flows.archive_datasets_flow.archive_datasets_flow","title":"<code>archive_datasets_flow(job_id, dataset_ids=None)</code>","text":"<p>Prefect flow to archive a list of datasets. Corresponds to a \"Job\" in Scicat. Runs the individual archivals of the single datasets as subflows and reports the overall job status to Scicat.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_ids</code> <code>List[str]</code> <p>description</p> <code>None</code> <code>job_id</code> <code>UUID</code> <p>description</p> required <p>Raises:</p> Type Description <code>e</code> <p>description</p> Source code in <code>backend/archiver/flows/archive_datasets_flow.py</code> <pre><code>@flow(\n    name=\"archive_datasetlist\",\n    log_prints=True,\n    flow_run_name=generate_flow_name_job_id,\n    on_failure=[on_job_flow_failure],\n    on_cancellation=[on_job_flow_cancellation],\n)\ndef archive_datasets_flow(job_id: UUID, dataset_ids: List[str] | None = None):\n    \"\"\"Prefect flow to archive a list of datasets. Corresponds to a \"Job\" in Scicat. Runs the individual archivals of the single datasets as subflows and reports\n    the overall job status to Scicat.\n\n    Args:\n        dataset_ids (List[str]): _description_\n        job_id (UUID): _description_\n\n    Raises:\n        e: _description_\n    \"\"\"\n    dataset_ids: List[str] = dataset_ids or []\n    access_token = get_scicat_access_token.submit()\n\n    job_update = update_scicat_archival_job_status.submit(\n        job_id=job_id,\n        status_code=SciCatClient.JOBSTATUSCODE.IN_PROGRESS,\n        status_message=SciCatClient.JOBSTATUSMESSAGE.JOB_IN_PROGRESS,\n        token=access_token,\n    )\n    job_update.result()\n\n    dataset_ids_future = get_job_datasetlist.submit(job_id=job_id, token=access_token)\n    dataset_ids = dataset_ids_future.result()\n\n    for id in dataset_ids:\n        archive_single_dataset_flow(dataset_id=id)\n\n    access_token = get_scicat_access_token.submit()\n\n    update_scicat_archival_job_status.submit(\n        job_id=job_id,\n        status_code=SciCatClient.JOBSTATUSCODE.FINISHED_SUCCESSFULLY,\n        status_message=SciCatClient.JOBSTATUSMESSAGE.JOB_FINISHED,\n        token=access_token,\n    ).result()\n</code></pre>"},{"location":"reference/flows/archive_datasets_flow/#flows.archive_datasets_flow.check_free_space_in_LTS","title":"<code>check_free_space_in_LTS()</code>","text":"<p>Prefect task to wait for free space in the LTS. Checks periodically if the condition for enough free space is fulfilled. Only one of these task runs at time; the others are only scheduled once this task has finished, i.e. there is enough space.</p> Source code in <code>backend/archiver/flows/archive_datasets_flow.py</code> <pre><code>@task(tags=[ConcurrencyLimits().LTS_FREE_TAG])\ndef check_free_space_in_LTS():\n    \"\"\"Prefect task to wait for free space in the LTS. Checks periodically if the condition for enough\n    free space is fulfilled. Only one of these task runs at time; the others are only scheduled once this task\n    has finished, i.e. there is enough space.\n    \"\"\"\n    asyncio.run(wait_for_free_space())\n</code></pre>"},{"location":"reference/flows/archive_datasets_flow/#flows.archive_datasets_flow.copy_datablock_from_LTS","title":"<code>copy_datablock_from_LTS(dataset_id, datablock)</code>","text":"<p>Prefect task to move a datablock (.tar file) to the LTS. Concurrency of this task is limited to 2 instances at the same time.</p> Source code in <code>backend/archiver/flows/archive_datasets_flow.py</code> <pre><code>@task(\n    task_run_name=generate_task_name_dataset,\n    tags=[ConcurrencyLimits().LTS_READ_TAG],\n    retries=5,\n    retry_delay_seconds=[60, 120, 240, 480, 960],\n)\ndef copy_datablock_from_LTS(dataset_id: str, datablock: DataBlock):\n    \"\"\"Prefect task to move a datablock (.tar file) to the LTS. Concurrency of this task is limited to 2 instances\n    at the same time.\n    \"\"\"\n    datablocks_operations.copy_file_from_LTS(dataset_id, datablock)\n</code></pre>"},{"location":"reference/flows/archive_datasets_flow/#flows.archive_datasets_flow.create_datablocks_flow","title":"<code>create_datablocks_flow(dataset_id)</code>","text":"<p>Prefect (sub-)flow to create datablocks (.tar files) for files of a dataset and register them in Scicat.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset id</p> required <p>Returns:</p> Type Description <code>List[DataBlock]</code> <p>List[DataBlock]: List of created and registered datablocks</p> Source code in <code>backend/archiver/flows/archive_datasets_flow.py</code> <pre><code>@flow(name=\"create_datablocks\", flow_run_name=generate_subflow_run_name_job_id_dataset_id)\ndef create_datablocks_flow(dataset_id: str) -&gt; List[DataBlock]:\n    \"\"\"Prefect (sub-)flow to create datablocks (.tar files) for files of a dataset and register them in Scicat.\n\n    Args:\n        dataset_id (str): Dataset id\n\n    Returns:\n        List[DataBlock]: List of created and registered datablocks\n    \"\"\"\n\n    scicat_token = get_scicat_access_token.submit()\n\n    dataset_update = update_scicat_archival_dataset_lifecycle.submit(\n        dataset_id=dataset_id,\n        status=SciCatClient.ARCHIVESTATUSMESSAGE.STARTED,\n        token=scicat_token,\n    )\n\n    orig_datablocks = get_origdatablocks.with_options(\n        on_failure=[partial(on_get_origdatablocks_error, dataset_id)]\n    ).submit(dataset_id=dataset_id, token=scicat_token, wait_for=[dataset_update])  # type: ignore\n\n    files = download_origdatablocks.submit(dataset_id=dataset_id, origDataBlocks=orig_datablocks)\n\n    tarfiles_future = create_tarfiles.submit(dataset_id, wait_for=[files])\n    datablocks_future = create_datablock_entries.submit(dataset_id, orig_datablocks, tarfiles_future)\n\n    # Prefect issue: https://github.com/PrefectHQ/prefect/issues/12028\n    # Exceptions are not propagated correctly\n    files.result()\n    tarfiles_future.result()\n    datablocks_future.result()\n\n    scicat_token = get_scicat_access_token.submit(wait_for=[datablocks_future])\n\n    register_future = register_datablocks.submit(\n        datablocks=datablocks_future,  # type: ignore\n        dataset_id=dataset_id,\n        token=scicat_token,\n    )\n\n    register_future.result()\n\n    return datablocks_future\n</code></pre>"},{"location":"reference/flows/archive_datasets_flow/#flows.archive_datasets_flow.move_data_to_LTS","title":"<code>move_data_to_LTS(dataset_id, datablock)</code>","text":"<p>Prefect task to move a datablock (.tar file) to the LTS. Concurrency of this task is limited to 2 instances at the same time.</p> Source code in <code>backend/archiver/flows/archive_datasets_flow.py</code> <pre><code>@task(task_run_name=generate_task_name_dataset, tags=[ConcurrencyLimits().LTS_WRITE_TAG])\ndef move_data_to_LTS(dataset_id: str, datablock: DataBlock):\n    \"\"\"Prefect task to move a datablock (.tar file) to the LTS. Concurrency of this task is limited to 2 instances\n    at the same time.\n    \"\"\"\n    datablocks_operations.move_data_to_LTS(dataset_id, datablock)\n</code></pre>"},{"location":"reference/flows/archive_datasets_flow/#flows.archive_datasets_flow.move_datablocks_to_lts_flow","title":"<code>move_datablocks_to_lts_flow(dataset_id, datablocks)</code>","text":"<p>Prefect (sub-)flow to move a datablock to the LTS. Implements the copying of data and verification via checksum.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>description</p> required <code>datablock</code> <code>DataBlock</code> <p>description</p> required Source code in <code>backend/archiver/flows/archive_datasets_flow.py</code> <pre><code>@flow(\n    name=\"move_datablocks_to_lts\",\n    log_prints=True,\n    flow_run_name=generate_subflow_run_name_job_id_dataset_id,\n)\ndef move_datablocks_to_lts_flow(dataset_id: str, datablocks: List[DataBlock]):\n    \"\"\"Prefect (sub-)flow to move a datablock to the LTS. Implements the copying of data and verification via checksum.\n\n    Args:\n        dataset_id (str): _description_\n        datablock (DataBlock): _description_\n    \"\"\"\n    tasks = []\n    all_tasks = []\n\n    for datablock in datablocks:\n        checksum = calculate_checksum.submit(dataset_id=dataset_id, datablock=datablock)\n        free_space = check_free_space_in_LTS.submit(wait_for=[checksum])\n\n        move = move_data_to_LTS.submit(dataset_id=dataset_id, datablock=datablock, wait_for=[free_space])  # type: ignore\n\n        getLogger().info(f\"Wait {Variables().ARCHIVER_LTS_WAIT_BEFORE_VERIFY_S}s before verifying datablock\")\n        sleep = sleep_for.submit(Variables().ARCHIVER_LTS_WAIT_BEFORE_VERIFY_S, wait_for=[move])\n\n        copy = copy_datablock_from_LTS.submit(dataset_id=dataset_id, datablock=datablock, wait_for=[sleep])\n\n        checksum_verification = verify_checksum.submit(\n            dataset_id=dataset_id, datablock=datablock, checksum=checksum, wait_for=[copy]\n        )\n\n        w = verify_datablock_in_verification.submit(\n            dataset_id=dataset_id, datablock=datablock, wait_for=[checksum_verification]\n        )  # type: ignore\n        tasks.append(w)\n\n        all_tasks.append(free_space)\n        all_tasks.append(checksum)\n        all_tasks.append(move)\n        all_tasks.append(sleep)\n        all_tasks.append(copy)\n        all_tasks.append(checksum_verification)\n        all_tasks.append(w)\n\n    wait_for_futures(tasks)\n\n    # this is necessary to propagate the errors of the tasks\n    for t in all_tasks:\n        t.result()\n</code></pre>"},{"location":"reference/flows/archive_datasets_flow/#flows.archive_datasets_flow.on_get_origdatablocks_error","title":"<code>on_get_origdatablocks_error(dataset_id, task, task_run, state)</code>","text":"<p>Callback for get_origdatablocks tasks. Reports a user error.</p> Source code in <code>backend/archiver/flows/archive_datasets_flow.py</code> <pre><code>def on_get_origdatablocks_error(dataset_id: str, task: Task, task_run: TaskRun, state: State):\n    \"\"\"Callback for get_origdatablocks tasks. Reports a user error.\"\"\"\n    scicat_token = get_scicat_access_token()\n    report_dataset_user_error(dataset_id, token=scicat_token)\n</code></pre>"},{"location":"reference/flows/archive_datasets_flow/#flows.archive_datasets_flow.sleep_for","title":"<code>sleep_for(time_in_seconds)</code>","text":"<p>Sleeps for a given amount of time. Required to wait for the LTS to update its internal state. Needs to be blocking as it should prevent the following task to run.</p> Source code in <code>backend/archiver/flows/archive_datasets_flow.py</code> <pre><code>@task(task_run_name=generate_sleep_for_task_name)\ndef sleep_for(time_in_seconds: int):\n    \"\"\"Sleeps for a given amount of time. Required to wait for the LTS to update its internal state.\n    Needs to be blocking as it should prevent the following task to run.\n    \"\"\"\n    time.sleep(time_in_seconds)\n</code></pre>"},{"location":"reference/flows/archive_datasets_flow/#flows.archive_datasets_flow.verify_datablock_in_verification","title":"<code>verify_datablock_in_verification(dataset_id, datablock)</code>","text":"<p>Prefect Task to verify a datablock in the LTS against a checksum. Task of this type run with no concurrency since the LTS does only allow limited concurrent access.</p> Source code in <code>backend/archiver/flows/archive_datasets_flow.py</code> <pre><code>@task(task_run_name=generate_task_name_dataset)\ndef verify_datablock_in_verification(dataset_id: str, datablock: DataBlock) -&gt; None:\n    \"\"\"Prefect Task to verify a datablock in the LTS against a checksum. Task of this type run with no concurrency since the LTS\n    does only allow limited concurrent access.\n    \"\"\"\n    datablocks_operations.verify_datablock_in_verification(dataset_id=dataset_id, datablock=datablock)\n</code></pre>"},{"location":"reference/flows/flow_utils/","title":"Flow utils","text":""},{"location":"reference/flows/flow_utils/#flows.flow_utils.DatasetError","title":"<code>DatasetError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception to report different error types to Scicat</p> Source code in <code>backend/archiver/flows/flow_utils.py</code> <pre><code>class DatasetError(Exception):\n    \"\"\"Custom exception to report different error types to Scicat\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/flows/flow_utils/#flows.flow_utils.StoragePaths","title":"<code>StoragePaths</code>","text":"<p>Helper class to create paths in scratch and LTS folder</p> Source code in <code>backend/archiver/flows/flow_utils.py</code> <pre><code>class StoragePaths:\n    \"\"\"Helper class to create paths in scratch and LTS folder\"\"\"\n\n    @staticmethod\n    def scratch_folder(dataset_id: str) -&gt; Path:\n        return StoragePaths.scratch_archival_root() / StoragePaths._relative_dataset_folder(dataset_id)\n\n    @staticmethod\n    def scratch_archival_root() -&gt; Path:\n        return Variables().ARCHIVER_SCRATCH_FOLDER / \"archival\"\n\n    @staticmethod\n    def _relative_dataset_folder(dataset_id: str) -&gt; Path:\n        return Path(\"openem-network\") / \"datasets\" / dataset_id\n\n    _relative_datablocks_folder: Path = Path(\"datablocks\")\n    _relative_raw_files_folder: Path = Path(\"raw_files\")\n\n    @staticmethod\n    def relative_datablocks_folder(dataset_id: str):\n        return StoragePaths._relative_dataset_folder(dataset_id) / StoragePaths._relative_datablocks_folder\n\n    @staticmethod\n    def relative_raw_files_folder(dataset_id: str):\n        return StoragePaths._relative_dataset_folder(dataset_id) / StoragePaths._relative_raw_files_folder\n\n    @staticmethod\n    def scratch_archival_datablocks_folder(dataset_id: str) -&gt; Path:\n        return StoragePaths.scratch_archival_root() / StoragePaths.relative_datablocks_folder(dataset_id)\n\n    @staticmethod\n    def scratch_archival_raw_files_folder(dataset_id: str) -&gt; Path:\n        return StoragePaths.scratch_archival_root() / StoragePaths.relative_raw_files_folder(dataset_id)\n\n    @staticmethod\n    def lts_datablocks_folder(dataset_id: str) -&gt; Path:\n        return Variables().LTS_STORAGE_ROOT / StoragePaths.relative_datablocks_folder(dataset_id)\n</code></pre>"},{"location":"reference/flows/flow_utils/#flows.flow_utils.SystemError","title":"<code>SystemError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception to report different error types to Scicat</p> Source code in <code>backend/archiver/flows/flow_utils.py</code> <pre><code>class SystemError(Exception):\n    \"\"\"Custom exception to report different error types to Scicat\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/flows/flow_utils/#flows.flow_utils.report_archival_error","title":"<code>report_archival_error(dataset_id, state, task_run, token)</code>","text":"<p>Report an error of an archival job of a dataset. Differntiates betwen \"DatasetError\" (User error, e.g. missing files) and SystemError (transient error).</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>dataset id</p> required <code>state</code> <code>State</code> <p>task run state</p> required <code>task_run</code> <code>TaskRun</code> <p>task run</p> required Source code in <code>backend/archiver/flows/flow_utils.py</code> <pre><code>def report_archival_error(dataset_id: str, state: State, task_run: TaskRun, token: SecretStr):\n    \"\"\"Report an error of an archival job of a dataset. Differntiates betwen \"DatasetError\" (User error, e.g. missing files)\n    and SystemError (transient error).\n\n    Args:\n        dataset_id (str): dataset id\n        state (State): task run state\n        task_run (TaskRun): task run\n    \"\"\"\n    try:\n        state.result()\n    except DatasetError:\n        report_dataset_user_error(dataset_id=dataset_id, token=token)\n    except SystemError:\n        report_dataset_system_error(dataset_id=dataset_id, token=token)\n    except Exception:\n        # TODO: add some info about unknown errors\n        report_dataset_system_error(dataset_id=dataset_id, token=token)\n</code></pre>"},{"location":"reference/flows/flow_utils/#flows.flow_utils.report_retrieval_error","title":"<code>report_retrieval_error(dataset_id, state, task_run, token)</code>","text":"<p>Report a retrieval error of a job of a dataset. Differentiates between \"DatasetError\" (User error, e.g. missing files) and SystemError (transient error).</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>dataset id</p> required <code>state</code> <code>State</code> <p>task run state</p> required <code>task_run</code> <code>TaskRun</code> <p>task run</p> required Source code in <code>backend/archiver/flows/flow_utils.py</code> <pre><code>def report_retrieval_error(dataset_id: str, state: State, task_run: TaskRun, token: SecretStr):\n    \"\"\"Report a retrieval error of a job of a dataset. Differentiates between \"DatasetError\" (User error, e.g. missing files)\n    and SystemError (transient error).\n\n    Args:\n        dataset_id (str): dataset id\n        state (State): task run state\n        task_run (TaskRun): task run\n    \"\"\"\n\n    report_dataset_retrieval_error(dataset_id=dataset_id, token=token)\n</code></pre>"},{"location":"reference/flows/mock_flows/","title":"Mock flows","text":""},{"location":"reference/flows/retrieve_datasets_flow/","title":"Retrieve datasets flow","text":""},{"location":"reference/flows/task_utils/","title":"Task utils","text":""},{"location":"reference/flows/tests/conftest/","title":"Conftest","text":""},{"location":"reference/flows/tests/conftest/#flows.tests.conftest.aws_and_s3_credentials","title":"<code>aws_and_s3_credentials()</code>","text":"<p>Mocked AWS Credentials for moto.</p> Source code in <code>backend/archiver/flows/tests/conftest.py</code> <pre><code>@pytest.fixture(scope=\"function\")\ndef aws_and_s3_credentials():\n    \"\"\"Mocked AWS Credentials for moto.\"\"\"\n    os.environ[\"AWS_ACCESS_KEY_ID\"] = \"testing\"\n    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"testing\"\n    os.environ[\"AWS_SECURITY_TOKEN\"] = \"testing\"\n    os.environ[\"AWS_SESSION_TOKEN\"] = \"testing\"\n    os.environ[\"AWS_DEFAULT_REGION\"] = \"eu-west-1\"\n    os.environ[\"MINIO_REGION\"] = \"eu-west-1\"\n    os.environ[\"MINIO_ENDPOINT\"] = \"endpoint:9000\"\n    os.environ[\"MINIO_EXTERNAL_ENDPOINT\"] = \"endpoint:9000\"\n</code></pre>"},{"location":"reference/flows/tests/conftest/#flows.tests.conftest.mocked_s3","title":"<code>mocked_s3(aws_and_s3_credentials)</code>","text":"<p>Mock all AWS interactions Requires you to create your own boto3 clients</p> Source code in <code>backend/archiver/flows/tests/conftest.py</code> <pre><code>@pytest.fixture(scope=\"function\")\ndef mocked_s3(aws_and_s3_credentials):\n    \"\"\"\n    Mock all AWS interactions\n    Requires you to create your own boto3 clients\n    \"\"\"\n    with mock_aws():\n        yield\n</code></pre>"},{"location":"reference/flows/tests/helpers/","title":"Helpers","text":""},{"location":"reference/flows/tests/scicat_unittest_mock/","title":"Scicat unittest mock","text":""},{"location":"reference/flows/tests/test_archival_flow/","title":"Test archival flow","text":""},{"location":"reference/flows/tests/test_retrieval_flow/","title":"Test retrieval flow","text":""},{"location":"reference/scicat/scicat_interface/","title":"Scicat interface","text":""},{"location":"reference/scicat/scicat_interface/#scicat.scicat_interface.SciCatClient","title":"<code>SciCatClient</code>","text":"Source code in <code>backend/archiver/scicat/scicat_interface.py</code> <pre><code>class SciCatClient:\n    class JOBSTATUSMESSAGE(StrEnum):\n        \"\"\"Human readible job status\n\n        Newly added property in Scicat /api/v4 allowing for more elaborate messages.\n        \"\"\"\n\n        JOB_IN_PROGRESS = \"jobStarted\"\n        JOB_UPDATED = \"jobUpdated\"\n        JOB_FINISHED = \"jobFinished\"\n\n    class JOBSTATUSCODE(StrEnum):\n        \"\"\"Job status code in Scicat /api/v4.\n\n        This was previously JobStatusMessage in /api/v3. Values are not restricted but follow the convention of PSI's archiver.\n\n        \"\"\"\n\n        IN_PROGRESS = \"inProgress\"\n        FINISHED_SUCCESSFULLY = \"finishedSuccessful\"\n        FINISHED_UNSUCCESSFULLY = \"finishedUnsuccessful\"\n        FINISHED_WITHDATASET_ERRORS = \"finishedWithDatasetErrors\"\n\n    class ARCHIVESTATUSMESSAGE(StrEnum):\n        STARTED = \"started\"\n        IS_ON_CENTRAL_DISK = \"isOnCentralDisk\"\n        DATASET_ON_ARCHIVEDISK = \"datasetOnArchiveDisk\"\n        SCHEDULE_ARCHIVE_JOB_FAILED = \"scheduleArchiveJobFailed\"\n        MISSING_FILES = \"missingFilesError\"\n\n    class RETRIEVESTATUSMESSAGE(StrEnum):\n        STARTED = \"started\"\n        DATASET_RETRIEVED = \"datasetRetrieved\"\n        DATASET_RETRIEVAL_FAILED = \"datasetRetrievalFailed\"\n\n    class JOBTYPE(StrEnum):\n        ARCHIVE = \"archive\"\n        RETRIEVE = \"retrieve\"\n\n    def __init__(\n        self,\n        endpoint: str = \"http://scicat.example.com\",\n        datasets_api_prefix: str = \"\",\n        api_prefix: str = \"\",\n        jobs_api_prefix: str = \"\",\n    ):\n        self._ENDPOINT = endpoint\n        self._API_PREFIX = api_prefix\n        self._DATASETS_API_PREFIX = datasets_api_prefix\n        self._JOBS_API_PREFIX = jobs_api_prefix\n\n        self._session = requests.Session()\n        retries = Retry(total=5, backoff_factor=1, status_forcelist=[502, 503, 504])\n        self._session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n\n    def _headers(self, token: SecretStr):\n        return {\n            \"Authorization\": f\"Bearer {token.get_secret_value()}\",\n            \"Content-Type\": \"application/json\",\n        }\n\n    def _safe_dataset_id(self, dataset_id: str):\n        return urllib.parse.quote(dataset_id, safe=\"\", encoding=None, errors=None)\n\n    def get_token(self) -&gt; str:\n        user = Blocks().SCICAT_USER\n        password = Blocks().SCICAT_PASSWORD\n\n        resp = self._session.post(\n            url=f\"{self._ENDPOINT}{self._API_PREFIX}/auth/login\",\n            data={\"username\": user, \"password\": password.get_secret_value()},\n        )\n        resp.raise_for_status()\n        return resp.json()[\"access_token\"]\n\n    @property\n    def API(self):\n        return self._API_PREFIX\n\n    @property\n    def JOBS_API_PREFIX(self):\n        return self._JOBS_API_PREFIX\n\n    @property\n    def DATASETS_API_PREFIX(self):\n        return self._DATASETS_API_PREFIX\n\n    @log\n    def update_job_status(\n        self,\n        job_id: UUID,\n        status_code: JOBSTATUSCODE,\n        status_message: JOBSTATUSMESSAGE,\n        job_result_object: JobResultObject | None,\n        token: SecretStr,\n    ) -&gt; None:\n        job = Job(jobStatusMessage=str(status_message), jobResultObject=job_result_object)\n\n        headers = self._headers(token)\n\n        result = self._session.patch(\n            f\"{self._ENDPOINT}{self.JOBS_API_PREFIX}/jobs/{job_id}\",\n            data=job.model_dump_json(exclude_none=True),\n            headers=headers,\n        )\n\n        # returns none if status_code is 200\n        result.raise_for_status()\n\n    @log\n    def update_archival_dataset_lifecycle(\n        self,\n        dataset_id: str,\n        status: ARCHIVESTATUSMESSAGE,\n        token: SecretStr,\n        archivable: bool | None = None,\n        retrievable: bool | None = None,\n    ) -&gt; None:\n        dataset = Dataset(\n            datasetlifecycle=DatasetLifecycle(\n                archiveStatusMessage=str(status),\n                archivable=archivable,\n                retrievable=retrievable,\n            )\n        )\n\n        headers = self._headers(token)\n        safe_dataset_id = self._safe_dataset_id(dataset_id)\n        result = self._session.patch(\n            f\"{self._ENDPOINT}{self.DATASETS_API_PREFIX}/datasets/{safe_dataset_id}\",\n            data=dataset.model_dump_json(exclude_none=True),\n            headers=headers,\n        )\n        # returns none if status_code is 200\n        result.raise_for_status()\n\n    @log\n    def update_retrieval_dataset_lifecycle(\n        self,\n        dataset_id: str,\n        status: RETRIEVESTATUSMESSAGE,\n        token: SecretStr,\n        archivable: bool | None = None,\n        retrievable: bool | None = None,\n    ) -&gt; None:\n        dataset = Dataset(\n            datasetlifecycle=DatasetLifecycle(\n                retrieveStatusMessage=str(status),\n                archivable=archivable,\n                retrievable=retrievable,\n            )\n        )\n        headers = self._headers(token)\n        safe_dataset_id = self._safe_dataset_id(dataset_id)\n        result = self._session.patch(\n            f\"{self._ENDPOINT}{self.DATASETS_API_PREFIX}/datasets/{safe_dataset_id}\",\n            data=dataset.model_dump_json(exclude_none=True),\n            headers=headers,\n        )\n        # returns none if status_code is 200\n        result.raise_for_status()\n\n    @log\n    def register_datablocks(self, dataset_id: str, data_blocks: List[DataBlock], token: SecretStr) -&gt; None:\n        headers = self._headers(token)\n        safe_dataset_id = self._safe_dataset_id(dataset_id)\n        for d in data_blocks:\n            result = self._session.post(\n                f\"{self._ENDPOINT}{self.DATASETS_API_PREFIX}/datasets/{safe_dataset_id}/datablocks\",\n                data=d.model_dump_json(exclude_none=True),\n                headers=headers,\n            )\n            # returns none if status_code is 200\n            result.raise_for_status()\n\n    @log\n    def delete_datablocks(self, dataset_id: str, data_blocks: List[DataBlock], token: SecretStr) -&gt; None:\n        headers = self._headers(token)\n        safe_dataset_id = self._safe_dataset_id(dataset_id)\n        for d in data_blocks:\n            result = self._session.delete(\n                f\"{self._ENDPOINT}{self.DATASETS_API_PREFIX}/datasets/{safe_dataset_id}/datablocks/{d.id}\",\n                headers=headers,\n            )\n            # returns none if status_code is 200\n            result.raise_for_status()\n\n    @log\n    def get_origdatablocks(self, dataset_id: str, token: SecretStr) -&gt; List[OrigDataBlock]:\n        headers = self._headers(token)\n        safe_dataset_id = self._safe_dataset_id(dataset_id)\n        result = self._session.get(\n            f\"{self._ENDPOINT}{self.DATASETS_API_PREFIX}/datasets/{safe_dataset_id}/origdatablocks\",\n            headers=headers,\n        )\n        # returns none if status_code is 200\n        result.raise_for_status()\n\n        origdatablocks: List[OrigDataBlock] = []\n        for r in result.json():\n            try:\n                origdatablocks.append(OrigDataBlock.model_validate(r))\n            except:\n                origdatablocks.append(OrigDataBlock.model_validate_json(r))\n        return origdatablocks\n\n    @log\n    def get_job_datasetlist(self, job_id: UUID, token: SecretStr) -&gt; List[str]:\n        headers = self._headers(token)\n        result = self._session.get(f\"{self._ENDPOINT}{self.JOBS_API_PREFIX}/jobs/{job_id}\", headers=headers)\n        # returns none if status_code is 200\n        result.raise_for_status()\n        # v3\n        datasets = result.json().get(\"datasetList\", [])\n        if not datasets:\n            # #v4\n            datasets = result.json().get(\"jobParams\")[\"datasetList\"]\n        final_list = [d[\"pid\"] for d in datasets]\n        return final_list\n\n    @log\n    def get_datablocks(self, dataset_id: str, token: SecretStr) -&gt; List[DataBlock]:\n        headers = self._headers(token)\n        safe_dataset_id = self._safe_dataset_id(dataset_id)\n        result = self._session.get(\n            f\"{self._ENDPOINT}{self.DATASETS_API_PREFIX}/datasets/{safe_dataset_id}/datablocks\",\n            headers=headers,\n        )\n        # returns none if status_code is 200\n        result.raise_for_status()\n\n        datablocks: List[DataBlock] = []\n        for r in result.json():\n            try:\n                datablocks.append(DataBlock.model_validate(r))\n            except:\n                datablocks.append(DataBlock.model_validate_json(r))\n        return datablocks\n</code></pre>"},{"location":"reference/scicat/scicat_interface/#scicat.scicat_interface.SciCatClient.JOBSTATUSCODE","title":"<code>JOBSTATUSCODE</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Job status code in Scicat /api/v4.</p> <p>This was previously JobStatusMessage in /api/v3. Values are not restricted but follow the convention of PSI's archiver.</p> Source code in <code>backend/archiver/scicat/scicat_interface.py</code> <pre><code>class JOBSTATUSCODE(StrEnum):\n    \"\"\"Job status code in Scicat /api/v4.\n\n    This was previously JobStatusMessage in /api/v3. Values are not restricted but follow the convention of PSI's archiver.\n\n    \"\"\"\n\n    IN_PROGRESS = \"inProgress\"\n    FINISHED_SUCCESSFULLY = \"finishedSuccessful\"\n    FINISHED_UNSUCCESSFULLY = \"finishedUnsuccessful\"\n    FINISHED_WITHDATASET_ERRORS = \"finishedWithDatasetErrors\"\n</code></pre>"},{"location":"reference/scicat/scicat_interface/#scicat.scicat_interface.SciCatClient.JOBSTATUSMESSAGE","title":"<code>JOBSTATUSMESSAGE</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Human readible job status</p> <p>Newly added property in Scicat /api/v4 allowing for more elaborate messages.</p> Source code in <code>backend/archiver/scicat/scicat_interface.py</code> <pre><code>class JOBSTATUSMESSAGE(StrEnum):\n    \"\"\"Human readible job status\n\n    Newly added property in Scicat /api/v4 allowing for more elaborate messages.\n    \"\"\"\n\n    JOB_IN_PROGRESS = \"jobStarted\"\n    JOB_UPDATED = \"jobUpdated\"\n    JOB_FINISHED = \"jobFinished\"\n</code></pre>"},{"location":"reference/scicat/scicat_tasks/","title":"Scicat tasks","text":""},{"location":"reference/tests/scicat_service_mock/","title":"Scicat service mock","text":""},{"location":"reference/tests/test_e2e/","title":"Test e2e","text":""},{"location":"reference/tests/test_e2e/#tests.test_e2e.test_end_to_end","title":"<code>test_end_to_end(scicat_token_setup, set_env, s3_client)</code>  <code>async</code>","text":"<p>Runs a full workflow, i.e. - creating and registering a dataset - archving (on test volume) - retrieving</p> <p>It checks the following endpoints involved for the final result - Prefect for flow execution - Minio for the created/retrieved data - Scicat for dataset and job creation</p> Source code in <code>backend/archiver/tests/test_e2e.py</code> <pre><code>@pytest.mark.skip(reason=\"Manually executed end to end test\")\n@pytest.mark.asyncio\nasync def test_end_to_end(scicat_token_setup, set_env, s3_client):\n    \"\"\"Runs a full workflow, i.e.\n    - creating and registering a dataset\n    - archving (on test volume)\n    - retrieving\n\n    It checks the following endpoints involved for the final result\n    - Prefect for flow execution\n    - Minio for the created/retrieved data\n    - Scicat for dataset and job creation\n\n    \"\"\"\n\n    # Create and register dataset -&gt; dataset pid\n    dataset_pid = await create_dataset()\n    dataset = await get_scicat_dataset(dataset_pid=dataset_pid, token=scicat_token_setup)\n    assert dataset is not None\n\n    # Verify Scicat datasetlifecycle\n    dataset_lifecycle = dataset.get(\"datasetlifecycle\")\n    assert dataset_lifecycle is not None\n    assert dataset_lifecycle.get(\"archiveStatusMessage\") == \"datasetCreated\"\n    assert dataset_lifecycle.get(\"archivable\")\n    assert not dataset_lifecycle.get(\"retrievable\")\n\n    # Verify datablocks in MINIO\n    orig_datablocks = list(\n        map(\n            lambda idx: s3_client.stat_object(\n                bucket=Bucket(\"landingzone\"),\n                filename=f\"openem-network/datasets/{dataset_pid}/raw_files/file_{idx}.bin\",\n            ),\n            range(9),\n        )\n    )\n    assert len(orig_datablocks) == 9\n\n    # trigger archive job in scicat\n    scicat_archival_job_id = await scicat_create_archival_job(dataset=dataset_pid, token=scicat_token_setup)\n\n    # Verify Scicat Job status\n    scicat_archival_job_status = await get_scicat_job(job_id=scicat_archival_job_id, token=scicat_token_setup)\n    assert scicat_archival_job_status is not None\n    assert scicat_archival_job_status.get(\"type\") == \"archive\"\n    assert (\n        scicat_archival_job_status.get(\"jobSstatusCode\") == \"jobCreated\"\n        or scicat_archival_job_status.get(\"jobStatusMessage\") == \"inProgress\"\n    )\n\n    time.sleep(10)\n    # Verify Prefect Flow\n    archival_flow_run_id = await find_flow_in_prefect(scicat_archival_job_id)\n    archival_state = await get_flow_result(flow_run_id=archival_flow_run_id)\n    assert archival_state is not None\n\n    # Verify Scicat Job status\n    scicat_archival_job_status = await get_scicat_job(job_id=scicat_archival_job_id, token=scicat_token_setup)\n    assert scicat_archival_job_status is not None\n    assert scicat_archival_job_status.get(\"type\") == \"archive\"\n    # v3\n    assert scicat_archival_job_status.get(\"jobStatusMessage\") == \"jobFinished\"\n    # v4\n    # assert scicat_archival_job_status.get(\"statusMessage\") == \"jobFinished\"\n    # assert scicat_archival_job_status.get(\"statusCode\") == \"finishedSuccessful\"\n\n    # Verify Scicat datasetlifecycle\n    dataset = await get_scicat_dataset(dataset_pid=dataset_pid, token=scicat_token_setup)\n    assert dataset is not None\n    dataset_lifecycle = dataset.get(\"datasetlifecycle\")\n    assert dataset_lifecycle is not None\n    assert dataset_lifecycle.get(\"archiveStatusMessage\") == \"datasetOnArchiveDisk\"\n    assert dataset_lifecycle.get(\"retrieveStatusMessage\") == \"\"\n    assert not dataset_lifecycle.get(\"archivable\")\n    assert dataset_lifecycle.get(\"retrievable\")\n\n    # trigger retrieval job in scicat\n    scicat_retrieval_job_id = await scicat_create_retrieval_job(dataset=dataset_pid, token=scicat_token_setup)\n\n    # Verify Scicat Job status\n    scicat_retrieval_job_status = await get_scicat_job(\n        job_id=scicat_retrieval_job_id, token=scicat_token_setup\n    )\n    assert scicat_retrieval_job_status is not None\n    assert scicat_retrieval_job_status.get(\"type\") == \"retrieve\"\n    assert (\n        scicat_retrieval_job_status.get(\"statusCode\") == \"jobCreated\"\n        or scicat_retrieval_job_status.get(\"statusMessage\") == \"inProgress\"\n    )\n\n    time.sleep(10)\n    # Verify Prefect Flow\n    retrieve_flow_run_id = await find_flow_in_prefect(scicat_retrieval_job_id)\n    retrieval_state = await get_flow_result(retrieve_flow_run_id)\n    assert retrieval_state is not None\n\n    # Verify Scicat Job status\n    scicat_retrieval_job_status = await get_scicat_job(\n        job_id=scicat_retrieval_job_id, token=scicat_token_setup\n    )\n    assert scicat_retrieval_job_status is not None\n    assert scicat_retrieval_job_status.get(\"type\") == \"retrieve\"\n\n    # v3\n    assert scicat_archival_job_status.get(\"jobStatusMessage\") == \"jobFinished\"\n    # v4\n    # assert scicat_retrieval_job_status.get(\"statusMessage\") == \"finishedSuccessful\"\n    # assert scicat_archival_job_status.get(\"statusCode\") == \"finishedSuccessful\"\n\n    assert scicat_retrieval_job_status.get(\"jobResultObject\") is not None\n    jobResult = scicat_retrieval_job_status.get(\"jobResultObject\").get(\"result\")\n    assert len(jobResult) == 1\n    assert jobResult[0].get(\"datasetId\") == dataset_pid\n    assert jobResult[0].get(\"url\") is not None\n    datablock_url = jobResult[0].get(\"url\")\n    datablock_name = jobResult[0].get(\"name\")\n\n    # verify file can be downloaded from MINIO via url in jobresult\n    with tempfile.TemporaryDirectory() as temp_dir:\n        dest_file: Path = Path(temp_dir) / datablock_name\n        urllib.request.urlretrieve(datablock_url, dest_file)\n        assert dest_file.exists()\n\n    # Verify retrieved datablock in MINIO\n    retrieved_datablock = s3_client.stat_object(\n        bucket=Bucket(\"retrieval\"),\n        filename=f\"openem-network/datasets/{dataset_pid}/datablocks/{dataset_pid}_0.tar\",\n    )\n    assert retrieved_datablock is not None\n    assert retrieved_datablock.Size &gt; 80 * 1024 * 1024\n\n    # Verify Scicat datasetlifecycle\n    dataset = await get_scicat_dataset(dataset_pid=dataset_pid, token=scicat_token_setup)\n    assert dataset is not None\n    dataset_lifecycle: Dict[Any, Any] = dataset.get(\"datasetlifecycle\")\n    assert dataset_lifecycle.get(\"retrieveStatusMessage\") == \"datasetRetrieved\"\n    # This is in fact a Scicat issue: fields in the datasetlifecycle are set to the default if not updated\n    # https://github.com/SciCatProject/scicat-backend-next/blob/release-jobs/src/datasets/datasets.service.ts#L273\n    # assert dataset_lifecycle.get(\"archiveStatusMessage\") == \"datasetOnArchiveDisk\"\n    assert not dataset_lifecycle.get(\"archivable\")\n    assert dataset_lifecycle.get(\"retrievable\")\n</code></pre>"},{"location":"reference/tests/test_scicat_tasks/","title":"Test scicat tasks","text":""},{"location":"reference/utils/datablocks/","title":"Datablocks","text":""},{"location":"reference/utils/datablocks/#utils.datablocks.calculate_md5_checksum","title":"<code>calculate_md5_checksum(filename, chunksize=2 ** 20)</code>","text":"<p>Calculate an md5 hash of a file</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>Path</code> <p>absolute or relative path to file</p> required <code>chunksize</code> <code>int</code> <p>default chunk size to calculate hash on. Defaults to 1024*1025.</p> <code>2 ** 20</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>hash as str</p> Source code in <code>backend/archiver/utils/datablocks.py</code> <pre><code>def calculate_md5_checksum(filename: Path, chunksize: int = 2**20) -&gt; str:\n    \"\"\"Calculate an md5 hash of a file\n\n    Args:\n        filename (Path): absolute or relative path to file\n        chunksize (int, optional): default chunk size to calculate hash on. Defaults to 1024*1025.\n\n    Returns:\n        str: hash as str\n    \"\"\"\n    import hashlib\n\n    m = hashlib.md5()\n    with open(filename, \"rb\") as f:\n        while chunk := f.read(chunksize):\n            m.update(chunk)\n    return m.hexdigest()\n</code></pre>"},{"location":"reference/utils/datablocks/#utils.datablocks.copy_file_to_folder","title":"<code>copy_file_to_folder(src_file, dst_folder)</code>","text":"<p>Copies a file to a destination folder (does not need to exist)</p> <p>Parameters:</p> Name Type Description Default <code>src_file</code> <code>Path</code> <p>Source file</p> required <code>dst_folder</code> <code>Path</code> <p>destination folder - needs to exist</p> required <p>Raises:</p> Type Description <code>SystemError</code> <p>raises if operation fails</p> Source code in <code>backend/archiver/utils/datablocks.py</code> <pre><code>@log\ndef copy_file_to_folder(src_file: Path, dst_folder: Path):\n    \"\"\"Copies a file to a destination folder (does not need to exist)\n\n    Args:\n        src_file (Path): Source file\n        dst_folder (Path): destination folder - needs to exist\n\n    Raises:\n        SystemError: raises if operation fails\n    \"\"\"\n    if not src_file.exists() or not src_file.is_file():\n        raise SystemError(f\"Source file {src_file} is not a file or does not exist\")\n    if dst_folder.is_file():\n        raise SystemError(f\"Destination folder {dst_folder} is not a folder\")\n\n    getLogger().info(f\"Start Copy operation. src:{src_file}, dst{dst_folder}\")\n\n    expected_dst_file = dst_folder / src_file.name\n\n    with subprocess.Popen(\n        [\n            \"rsync\",\n            \"-rcvh\",  # r: recursive, c: checksum, v: verbose, h: human readable format\n            \"--stats\",  # file transfer stats\n            \"--no-perms\",  # don't preserve the file permissions of the source files\n            \"--no-owner\",  # don't preserve the owner\n            \"--no-group\",  # don't preserve the group ownership\n            \"--mkpath\",\n            str(src_file),\n            str(dst_folder),\n        ],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n        universal_newlines=True,\n    ) as popen:\n        for line in popen.stdout:\n            getLogger().info(line)\n\n        popen.stdout.close()\n        return_code = popen.wait()\n        if return_code &gt; 0:\n            getLogger().error(f\"Finished with return code : {return_code}\")\n        else:\n            getLogger().info(f\"Finished with return code : {return_code}\")\n\n    if not expected_dst_file.exists():\n        raise SystemError(f\"Copying did not produce file {expected_dst_file}\")\n</code></pre>"},{"location":"reference/utils/datablocks/#utils.datablocks.create_datablock_entries","title":"<code>create_datablock_entries(dataset_id, folder, origDataBlocks, tar_infos, progress_callback=None)</code>","text":"<p>Create datablock entries compliant with schema provided by scicat</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset identifier</p> required <code>folder</code> <code>Path</code> <p>description</p> required <code>origDataBlocks</code> <code>List[OrigDataBlock]</code> <p>description</p> required <code>tarballs</code> <code>List[Path]</code> <p>description</p> required <p>Returns:</p> Type Description <code>List[DataBlock]</code> <p>List[DataBlock]: description</p> Source code in <code>backend/archiver/utils/datablocks.py</code> <pre><code>@log\ndef create_datablock_entries(\n    dataset_id: str,\n    folder: Path,\n    origDataBlocks: List[OrigDataBlock],\n    tar_infos: List[ArchiveInfo],\n    progress_callback: Callable[[float], None] = None,\n) -&gt; List[DataBlock]:\n    \"\"\"Create datablock entries compliant with schema provided by scicat\n\n    Args:\n        dataset_id (str): Dataset identifier\n        folder (Path): _description_\n        origDataBlocks (List[OrigDataBlock]): _description_\n        tarballs (List[Path]): _description_\n\n    Returns:\n        List[DataBlock]: _description_\n    \"\"\"\n\n    version = 1.0\n\n    total_file_count = 0\n    for b in origDataBlocks:\n        total_file_count += len(b.dataFileList)\n\n    file_count = 0\n\n    datablocks: List[DataBlock] = []\n\n    for idx, tar in enumerate(tar_infos):\n        # TODO: is it necessary to use any datablock information?\n        o = origDataBlocks[0]\n\n        data_file_list: List[DataFile] = []\n\n        tar_path = folder / tar.path\n\n        tarball = tarfile.open(tar_path)\n\n        def create_datafile_list_entry(tar_info: tarfile.TarInfo) -&gt; DataFile:\n            checksum = calculate_md5_checksum(\n                StoragePaths.scratch_archival_raw_files_folder(dataset_id) / tar_info.path\n            )\n\n            return DataFile(\n                path=tar_info.path,\n                size=tar_info.size,\n                chk=checksum,\n                uid=str(tar_info.uid),\n                gid=str(tar_info.gid),\n                perm=str(tar_info.mode),\n                time=str(datetime.datetime.now(datetime.UTC).isoformat()),\n            )\n\n        with ThreadPoolExecutor(max_workers=Variables().ARCHIVER_NUM_WORKERS) as executor:\n            future_to_key = {\n                executor.submit(create_datafile_list_entry, tar_info): tar_info\n                for tar_info in tarball.getmembers()\n            }\n\n            for future in as_completed(future_to_key):\n                exception = future.exception()\n\n                if not exception:\n                    data_file_list.append(future.result())\n                    file_count += 1\n                    if progress_callback:\n                        progress_callback(file_count / total_file_count)\n                else:\n                    raise exception\n\n        datablocks.append(\n            DataBlock(\n                archiveId=str(StoragePaths.relative_datablocks_folder(dataset_id) / tar_path.name),\n                size=tar.unpackedSize,\n                packedSize=tar.packedSize,\n                chkAlg=\"md5\",\n                version=str(version),\n                dataFileList=data_file_list,\n                rawDatasetId=o.rawdatasetId,\n                derivedDatasetId=o.derivedDatasetId,\n            )\n        )\n\n    return datablocks\n</code></pre>"},{"location":"reference/utils/datablocks/#utils.datablocks.create_tarfiles","title":"<code>create_tarfiles(dataset_id, src_folder, dst_folder, target_size, progress_callback=None)</code>","text":"<p>Create datablocks, i.e. .tar files, from all files in a folder. Folder structures are kept and symlnks not resolved. The created tar files will be named according to the dataset they belong to.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>dataset identifier</p> required <code>src_folder</code> <code>Path</code> <p>source folder to find files to create tars from</p> required <code>dst_folder</code> <code>Path</code> <p>destination folder to write the tar files to</p> required <code>target_size</code> <code>int</code> <p>Target size of the tar file. This is the unpacked size of the files.</p> required <p>Returns:</p> Type Description <code>List[ArchiveInfo]</code> <p>List[Path]: description</p> Source code in <code>backend/archiver/utils/datablocks.py</code> <pre><code>@log_debug\ndef create_tarfiles(\n    dataset_id: str,\n    src_folder: Path,\n    dst_folder: Path,\n    target_size: int,\n    progress_callback: Callable[[float], None] = None,\n) -&gt; List[ArchiveInfo]:\n    \"\"\"Create datablocks, i.e. .tar files, from all files in a folder. Folder structures are kept and symlnks not resolved.\n    The created tar files will be named according to the dataset they belong to.\n\n    Args:\n        dataset_id (str): dataset identifier\n        src_folder (Path): source folder to find files to create tars from\n        dst_folder (Path): destination folder to write the tar files to\n        target_size (int, optional): Target size of the tar file. This is the unpacked size of the files.\n\n    Returns:\n        List[Path]: _description_\n    \"\"\"\n\n    # TODO: corner case: target size &lt; file size\n    tarballs: List[ArchiveInfo] = []\n    tar_name = dataset_id.replace(\"/\", \"-\")\n\n    if not any(Path(src_folder).iterdir()):\n        raise SystemError(f\"Empty folder {src_folder} found.\")\n\n    total_file_count = count_files(src_folder)\n    current_file_count = 0\n\n    def create_tar(idx: int, files: List) -&gt; ArchiveInfo:\n        current_tar_info = ArchiveInfo(\n            unpackedSize=0,\n            packedSize=0,\n            path=Path(dst_folder / Path(f\"{tar_name}_{idx}.tar\")),\n            fileCount=len(files),\n        )\n        current_tarfile: tarfile.TarFile = tarfile.open(current_tar_info.path, \"w\")\n        for relative_file_path in files:\n            full_path = src_folder.joinpath(relative_file_path)\n            current_tar_info.unpackedSize += full_path.stat().st_size\n            current_tarfile.add(name=full_path, arcname=relative_file_path)\n\n        current_tarfile.close()\n        current_tar_info.packedSize = current_tar_info.path.stat().st_size\n        return current_tar_info\n\n    with ThreadPoolExecutor(max_workers=Variables().ARCHIVER_NUM_WORKERS) as executor:\n        future_to_key = {\n            executor.submit(create_tar, idx, files): (idx, files)\n            for (idx, files) in partition_files_flat(src_folder, target_size)\n        }\n        for future in as_completed(future_to_key):\n            exception = future.exception()\n\n            if not exception:\n                archive_info = future.result()\n                tarballs.append(archive_info)\n                if progress_callback:\n                    current_file_count += archive_info.fileCount\n                    progress_callback(current_file_count / total_file_count)\n            else:\n                raise exception\n\n    return tarballs\n</code></pre>"},{"location":"reference/utils/datablocks/#utils.datablocks.download_object_from_s3","title":"<code>download_object_from_s3(client, bucket, folder, object_name, target_path)</code>","text":"<p>Download an object from S3 storage.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>Bucket</code> <p>Bucket to look for file</p> required <code>folder</code> <code>Path</code> <p>s3 prefix for object</p> required <code>object_name</code> <code>str</code> <p>object name, no prefix</p> required <code>target_path</code> <code>Path</code> <p>absolute or relative path for the file to be created</p> required Source code in <code>backend/archiver/utils/datablocks.py</code> <pre><code>@log_debug\ndef download_object_from_s3(\n    client: S3Storage, bucket: Bucket, folder: Path, object_name: str, target_path: Path\n):\n    \"\"\"Download an object from S3 storage.\n\n    Args:\n        bucket (Bucket): Bucket to look for file\n        folder (Path): s3 prefix for object\n        object_name (str): object name, no prefix\n        target_path (Path): absolute or relative path for the file to be created\n    \"\"\"\n    client.fget_object(\n        bucket=bucket,\n        folder=str(folder),\n        object_name=object_name,\n        target_path=target_path,\n    )\n</code></pre>"},{"location":"reference/utils/datablocks/#utils.datablocks.download_objects_from_s3","title":"<code>download_objects_from_s3(client, prefix, bucket, destination_folder, progress_callback)</code>","text":"<p>Download objects form s3 storage to folder</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>Path</code> <p>S3 prefix</p> required <code>bucket</code> <code>Bucket</code> <p>s3 bucket</p> required <code>destination_folder</code> <code>Path</code> <p>Target folder. Will be created if it does not exist.</p> required <p>Returns:</p> Type Description <code>List[Path]</code> <p>List[Path]: List of paths of created files</p> Source code in <code>backend/archiver/utils/datablocks.py</code> <pre><code>@log\ndef download_objects_from_s3(\n    client: S3Storage, prefix: Path, bucket: Bucket, destination_folder: Path, progress_callback\n) -&gt; List[Path]:\n    \"\"\"Download objects form s3 storage to folder\n\n    Args:\n        prefix (Path): S3 prefix\n        bucket (Bucket): s3 bucket\n        destination_folder (Path): Target folder. Will be created if it does not exist.\n\n    Returns:\n        List[Path]: List of paths of created files\n    \"\"\"\n    destination_folder.mkdir(parents=True, exist_ok=True)\n\n    files = client.download_objects(\n        minio_prefix=prefix,\n        bucket=bucket,\n        destination_folder=destination_folder,\n        progress_callback=progress_callback,\n    )\n\n    if len(files) == 0:\n        raise SystemError(f\"No files found in bucket {bucket.name} at {prefix}\")\n\n    return files\n</code></pre>"},{"location":"reference/utils/datablocks/#utils.datablocks.list_datablocks","title":"<code>list_datablocks(client, prefix, bucket)</code>","text":"<p>List all objects in s3 bucket and path</p> <p>Parameters:</p> Name Type Description Default <code>minio_prefix</code> <code>Path</code> <p>prefix for files to be listed</p> required <code>bucket</code> <code>Bucket</code> <p>s3 bucket</p> required <p>Returns:</p> Name Type Description <code>_type_</code> <code>List[ListedObject]</code> <p>Iterator to objects</p> Source code in <code>backend/archiver/utils/datablocks.py</code> <pre><code>@log\ndef list_datablocks(client: S3Storage, prefix: Path, bucket: Bucket) -&gt; List[S3Storage.ListedObject]:\n    \"\"\"List all objects in s3 bucket and path\n\n    Args:\n        minio_prefix (Path): prefix for files to be listed\n        bucket (Bucket): s3 bucket\n\n    Returns:\n        _type_: Iterator to objects\n    \"\"\"\n    return client.list_objects(bucket, str(prefix))\n</code></pre>"},{"location":"reference/utils/datablocks/#utils.datablocks.partition_files_flat","title":"<code>partition_files_flat(folder, target_size_bytes)</code>","text":"<p>Partitions files in folder into groups such that all the files in a group combined have a target_size_bytes size at maximum. Folders are not treated recursively</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>Path</code> <p>Folder to partition files in</p> required <code>target_size_bytes</code> <code>int</code> <p>maximum size of grouped files</p> required <p>Yields:</p> Type Description <code>List[Path]</code> <p>Generator[List[Path], None, None]: List of paths with maximum size</p> Source code in <code>backend/archiver/utils/datablocks.py</code> <pre><code>def partition_files_flat(folder: Path, target_size_bytes: int) -&gt; Generator[List[Path], None, None]:\n    \"\"\"Partitions files in folder into groups such that all the files in a group combined\n    have a target_size_bytes size at maximum. Folders are not treated recursively\n\n    Args:\n        folder (Path): Folder to partition files in\n        target_size_bytes (int): maximum size of grouped files\n\n    Yields:\n        Generator[List[Path], None, None]: List of paths with maximum size\n    \"\"\"\n\n    if not folder.is_dir():\n        yield None\n\n    part: List[Path] = []\n    size = 0\n    idx = 0\n    for dirpath, dirnames, filenames in os.walk(folder):\n        for filename in filenames:\n            filepath = Path(os.path.join(dirpath, filename))\n            if size + os.path.getsize(filepath) &gt; target_size_bytes:\n                yield (idx, part)\n                part = []\n                size = 0\n                idx = idx + 1\n            part.append(filepath.relative_to(folder))\n            size = size + os.path.getsize(filepath)\n\n    yield (idx, part)\n</code></pre>"},{"location":"reference/utils/datablocks/#utils.datablocks.sufficient_free_space_on_lts","title":"<code>sufficient_free_space_on_lts()</code>","text":"<p>Checks for free space on configured LTS storage with respect to configured free space percentage.</p> <p>Returns:</p> Name Type Description <code>boolean</code> <p>condition of eneough free space satisfied</p> Source code in <code>backend/archiver/utils/datablocks.py</code> <pre><code>@log\ndef sufficient_free_space_on_lts():\n    \"\"\"Checks for free space on configured LTS storage with respect to configured free space percentage.\n\n    Returns:\n        boolean: condition of eneough free space satisfied\n    \"\"\"\n\n    path = Variables().LTS_STORAGE_ROOT\n    stat = shutil.disk_usage(path)\n    free_percentage = 100.0 * stat.free / stat.total\n    getLogger().info(\n        f\"LTS free space:{free_percentage:.2}%, expected: {Variables().LTS_FREE_SPACE_PERCENTAGE:.2}%\"\n    )\n    return free_percentage &gt;= Variables().LTS_FREE_SPACE_PERCENTAGE\n</code></pre>"},{"location":"reference/utils/datablocks/#utils.datablocks.wait_for_file_accessible","title":"<code>wait_for_file_accessible(file, timeout_s=360)</code>  <code>async</code>","text":"<p>Returns:</p> Source code in <code>backend/archiver/utils/datablocks.py</code> <pre><code>@log\nasync def wait_for_file_accessible(file: Path, timeout_s=360):\n    \"\"\"\n    Returns:\n    \"\"\"\n    total_time_waited_s = 0\n    while not os.access(path=file, mode=os.R_OK):\n        seconds_to_wait = 30\n        getLogger().info(f\"File {file} currently not available. Trying again in {seconds_to_wait} seconds.\")\n        await asyncio.sleep(seconds_to_wait)\n        total_time_waited_s += seconds_to_wait\n        if total_time_waited_s &gt; timeout_s:\n            raise SystemError(f\"File f{file} was not accessible within {timeout_s} seconds\")\n\n    getLogger().info(f\"File {file} accessible.\")\n\n    return True\n</code></pre>"},{"location":"reference/utils/datablocks/#utils.datablocks.wait_for_free_space","title":"<code>wait_for_free_space()</code>  <code>async</code>","text":"<p>Asynchronous wait until there is enough free space. Waits in linear intervals to check for free space</p> <pre><code>TODO: add exponential backoff for waiting time\n</code></pre> <p>Returns:</p> Name Type Description <code>boolean</code> <p>Returns True once there is enough free space</p> Source code in <code>backend/archiver/utils/datablocks.py</code> <pre><code>@log\nasync def wait_for_free_space():\n    \"\"\"Asynchronous wait until there is enough free space. Waits in linear intervals to check for free space\n\n        TODO: add exponential backoff for waiting time\n\n    Returns:\n        boolean: Returns True once there is enough free space\n    \"\"\"\n    while not sufficient_free_space_on_lts():\n        seconds_to_wait = 30\n        getLogger().info(f\"Not enough free space. Waiting for {seconds_to_wait}s\")\n        await asyncio.sleep(seconds_to_wait)\n\n    return True\n</code></pre>"},{"location":"reference/utils/log/","title":"Log","text":""},{"location":"reference/utils/log/#utils.log.getLogger","title":"<code>getLogger()</code>","text":"<p>Returns an logger depending on the environment: - Testing: returns a logger from the logging package - Prefect Flow: returns the logger from the prefect flow run context - Prefect error callback: there might not be a logger available, falls back to     a logger from logging package</p> Source code in <code>backend/archiver/utils/log.py</code> <pre><code>def getLogger():\n    \"\"\"\n    Returns an logger depending on the environment:\n    - Testing: returns a logger from the logging package\n    - Prefect Flow: returns the logger from the prefect flow run context\n    - Prefect error callback: there might not be a logger available, falls back to\n        a logger from logging package\n    \"\"\"\n    if \"PYTEST_CURRENT_TEST\" in os.environ:\n        return logging.getLogger(name=\"TestLogger\")\n    else:\n        try:\n            prefect_logger = prefect.get_run_logger()\n            return prefect_logger\n        except MissingContextError:\n            return logging.getLogger(name=\"FallbackLogger\")\n        except Exception:\n            return logging.getLogger(name=\"FallbackLogger\")\n</code></pre>"},{"location":"reference/utils/model/","title":"Model","text":""},{"location":"reference/utils/s3_storage_interface/","title":"S3 storage interface","text":""},{"location":"reference/utils/s3_storage_interface/#utils.s3_storage_interface.S3Storage","title":"<code>S3Storage</code>","text":"Source code in <code>backend/archiver/utils/s3_storage_interface.py</code> <pre><code>class S3Storage:\n    def __init__(self, url: str, user: str, password: SecretStr, region: str):\n        self._URL = url\n        self._USER = user\n        self._PASSWORD = password\n        self._REGION = region\n\n        self.STAGING_BUCKET: Bucket = Bucket.staging_bucket()\n        self.RETRIEVAL_BUCKET: Bucket = Bucket.retrieval_bucket()\n        self.LANDINGZONE_BUCKET: Bucket = Bucket.landingzone_bucket()\n\n        self._minio = boto3.client(\n            \"s3\",\n            endpoint_url=f\"https://{self._URL}\" if self._URL is not None and self._URL != \"\" else None,\n            aws_access_key_id=self._USER.strip(),\n            aws_secret_access_key=self._PASSWORD.get_secret_value().strip(),\n            region_name=self._REGION,\n            config=Config(signature_version=\"s3v4\", max_pool_connections=32),\n        )\n\n        self._external_minio = boto3.client(\n            \"s3\",\n            endpoint_url=f\"https://{Variables().MINIO_EXTERNAL_ENDPOINT}\",\n            aws_access_key_id=self._USER.strip(),\n            aws_secret_access_key=self._PASSWORD.get_secret_value().strip(),\n            region_name=self._REGION,\n            config=Config(signature_version=\"s3v4\", max_pool_connections=32),\n        )\n        self._resource = boto3.resource(\n            \"s3\",\n            endpoint_url=f\"https://{self._URL}\" if self._URL is not None and self._URL != \"\" else None,\n            aws_access_key_id=self._USER.strip(),\n            aws_secret_access_key=self._PASSWORD.get_secret_value().strip(),\n            region_name=self._REGION,\n            config=Config(signature_version=\"s3v4\"),\n        )\n\n    def __eq__(self, value: object) -&gt; bool:\n        if not isinstance(value, S3Storage):\n            return False\n        return self._URL == value._URL and self._USER == value._USER and self._REGION == value._REGION\n\n    @property\n    def url(self):\n        return self._URL\n\n    @log_debug\n    def get_presigned_url(self, bucket: Bucket, filename: str) -&gt; str:\n        days_to_seconds = 60 * 60 * 24\n\n\n        presigned_url = self._external_minio.generate_presigned_url(\n            \"get_object\",\n            Params={\"Bucket\": bucket.name, \"Key\": filename},\n            ExpiresIn=Variables().MINIO_URL_EXPIRATION_DAYS\n            * days_to_seconds,  # URL expiration time in seconds\n        )\n        return presigned_url\n\n    @dataclass\n    class StatInfo:\n        Size: int\n\n    @log_debug\n    def reset_expiry_date(self, bucket_name: str, filename: str, retention_period_days: int) -&gt; None:\n        new_expiration_date = datetime.datetime.now(datetime.timezone.utc) + datetime.timedelta(\n            days=retention_period_days\n        )\n\n        # the only way to reset the expiration date is to copy the object to itself apparently\n        copy_source = {\"Bucket\": bucket_name, \"Key\": filename}\n        self._minio.copy_object(\n            Bucket=bucket_name,\n            Key=filename,\n            CopySource=copy_source,\n            MetadataDirective=\"REPLACE\",  # This is important to replace metadata\n            Expires=new_expiration_date.isoformat(),\n        )\n\n    @log_debug\n    def stat_object(self, bucket: Bucket, filename: str) -&gt; StatInfo | None:\n        try:\n            object = self._minio.head_object(Bucket=bucket.name, Key=filename)\n            return S3Storage.StatInfo(Size=object[\"ContentLength\"])\n        except:\n            return None\n\n    @log_debug\n    def fget_object(self, bucket: Bucket, folder: str, object_name: str, target_path: Path):\n        self._minio.download_file(Bucket=bucket.name, Key=object_name, Filename=str(target_path.absolute()))\n\n    @log\n    def download_objects(\n        self,\n        minio_prefix: Path,\n        bucket: Bucket,\n        destination_folder: Path,\n        progress_callback: Callable[[float], None] | None = None,\n    ) -&gt; List[Path]:\n        remote_bucket = self._resource.Bucket(bucket.name)\n        objs = remote_bucket.objects.filter(Prefix=str(minio_prefix))\n\n        files: List[Path] = []\n\n        count = 0\n\n        with ThreadPoolExecutor(max_workers=Variables().ARCHIVER_NUM_WORKERS) as executor:\n            future_to_key = {\n                executor.submit(\n                    download_file, self._minio, key, minio_prefix, destination_folder, bucket\n                ): key\n                for key in objs\n            }\n\n            for future in as_completed(future_to_key):\n                count = count + 1\n                if progress_callback:\n                    progress_callback(count)\n                exception = future.exception()\n\n                if not exception:\n                    files.append(future.result())\n                else:\n                    raise exception\n\n        return files\n\n    @dataclass\n    class ListedObject:\n        Name: str\n\n    @log_debug\n    def list_objects(self, bucket: Bucket, folder: str | None = None) -&gt; List[S3Storage.ListedObject]:\n        \"\"\"Lists up to 1000 objects in a bucket. This is an s3 limitation\"\"\"\n        f = folder or \"\"\n        remote_bucket = self._resource.Bucket(bucket.name)\n        objs = remote_bucket.objects.filter(Prefix=f)\n\n        objects: List[S3Storage.ListedObject] = []\n        for obj in objs:\n            objects.append(S3Storage.ListedObject(Name=obj.key))\n\n        return objects\n\n    @log\n    def fput_object(self, source_file: Path, destination_file: Path, bucket: Bucket):\n        self._minio.upload_file(\n            Bucket=bucket.name,\n            Key=str(destination_file),\n            Filename=str(source_file),\n            ExtraArgs={},\n            Config=TransferConfig(\n                multipart_threshold=64 * 1024 * 1024,\n                multipart_chunksize=64 * 1024 * 1024,\n            ),\n        )\n\n    @log\n    def delete_objects(self, minio_prefix: Path, bucket: Bucket) -&gt; None:\n        remote_bucket = self._resource.Bucket(bucket.name)\n        objs = remote_bucket.objects.filter(Prefix=str(minio_prefix))\n\n        for obj in objs:\n            self._minio.delete_object(Bucket=bucket.name, Key=obj.key)\n</code></pre>"},{"location":"reference/utils/s3_storage_interface/#utils.s3_storage_interface.S3Storage.list_objects","title":"<code>list_objects(bucket, folder=None)</code>","text":"<p>Lists up to 1000 objects in a bucket. This is an s3 limitation</p> Source code in <code>backend/archiver/utils/s3_storage_interface.py</code> <pre><code>@log_debug\ndef list_objects(self, bucket: Bucket, folder: str | None = None) -&gt; List[S3Storage.ListedObject]:\n    \"\"\"Lists up to 1000 objects in a bucket. This is an s3 limitation\"\"\"\n    f = folder or \"\"\n    remote_bucket = self._resource.Bucket(bucket.name)\n    objs = remote_bucket.objects.filter(Prefix=f)\n\n    objects: List[S3Storage.ListedObject] = []\n    for obj in objs:\n        objects.append(S3Storage.ListedObject(Name=obj.key))\n\n    return objects\n</code></pre>"},{"location":"reference/utils/script_generation/","title":"Script generation","text":""},{"location":"reference/utils/tests/test_datablocks/","title":"Test datablocks","text":""},{"location":"reference/utils/tests/test_s3/","title":"Test s3","text":""},{"location":"reference/utils/tests/test_s3/#utils.tests.test_s3.aws_credentials","title":"<code>aws_credentials()</code>","text":"<p>Mocked AWS Credentials for moto.</p> Source code in <code>backend/archiver/utils/tests/test_s3.py</code> <pre><code>@pytest.fixture(scope=\"function\")\ndef aws_credentials():\n    \"\"\"Mocked AWS Credentials for moto.\"\"\"\n    os.environ[\"AWS_ACCESS_KEY_ID\"] = \"testing\"\n    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"testing\"\n    os.environ[\"AWS_SECURITY_TOKEN\"] = \"testing\"\n    os.environ[\"AWS_SESSION_TOKEN\"] = \"testing\"\n    os.environ[\"AWS_DEFAULT_REGION\"] = \"eu-west-1\"\n</code></pre>"},{"location":"reference/utils/tests/test_s3/#utils.tests.test_s3.s3","title":"<code>s3(aws_credentials)</code>","text":"<p>Return a mocked S3 client</p> Source code in <code>backend/archiver/utils/tests/test_s3.py</code> <pre><code>@pytest.fixture(scope=\"function\")\ndef s3(aws_credentials):\n    \"\"\"\n    Return a mocked S3 client\n    \"\"\"\n    with mock_aws():\n        user = \"user\"\n        password = \"pass\"\n        region = \"eu-west-1\"\n        client = boto3.client(\n            \"s3\",\n        )\n\n        location = {\"LocationConstraint\": region}\n        client.create_bucket(Bucket=\"landingzone\", CreateBucketConfiguration=location)\n        client.put_object(Bucket=\"landingzone\", Key=\"test-file\", Body=\"asdf\")\n\n        yield S3Storage(url=None, user=user, password=SecretStr(password), region=region)  # type: ignore\n</code></pre>"}]}